#!/home/gh/python/venv_py311/bin/python3
"""
AI Video Processor - GPU-Optimiert f√ºr Tesla P4
Verarbeitet vorhandene Video/Bild-Dateien mit vollst√§ndiger AI-Analyse
Gesichtserkennung mit Face-Cropping, Objekt-Detektion, Szenen-Erkennung

OPTIMIERT f√ºr Tesla P4 mit InsightFace GPU-Support
"""

import os
import sys

# KRITISCH: Environment-Variablen ZUERST setzen (vor allen AI-Imports!)
os.environ['ORT_DISABLE_CUDNN_FRONTEND'] = '1'
os.environ['CUDA_MODULE_LOADING'] = 'LAZY'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import time
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any
import re
import json

# Database imports
import pymysql

# AI/ML imports
import cv2
import numpy as np
from ultralytics import YOLO
import torch

# Configuration
MEDIA_BASE_PATH = "/var/www/web1"
ANNOTATED_OUTPUT_PATH = "/var/www/web1/annotated"
FACES_OUTPUT_PATH = "/var/www/web1/faces"
DB_CONFIG = {
    'database': 'wagodb',
    'user': 'gh',
    'password': 'a12345'
}

# AI Model paths
YOLO_MODEL_PATH = "/opt/models/yolov8l.pt"  # Large model for better accuracy
KNOWN_FACES_DIR = "/opt/known_faces"

# Logging Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./logs/watchdog.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def check_gpu_availability():
    """√úberpr√ºft GPU-Verf√ºgbarkeit und zeigt Details - Tesla P4 Erkennung"""
    logger.info("=" * 70)
    logger.info("GPU STATUS CHECK")
    logger.info("=" * 70)
    
    # Python Environment Info
    logger.info(f"Python Executable: {sys.executable}")
    logger.info(f"Python Version: {sys.version}")
    
    # CUDA verf√ºgbar?
    cuda_available = torch.cuda.is_available()
    logger.info(f"CUDA verf√ºgbar: {cuda_available}")
    
    if cuda_available:
        # GPU Details
        gpu_count = torch.cuda.device_count()
        logger.info(f"Anzahl GPUs: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            logger.info(f"GPU {i}: {gpu_name} ({gpu_memory:.2f} GB)")
            
            # Tesla P4 Detection
            if "Tesla P4" in gpu_name or "P4" in gpu_name:
                logger.info("‚úì Tesla P4 erkannt - optimale Einstellungen werden verwendet")
                logger.info("  - FP16 (Half Precision) aktiviert")
                logger.info("  - Batch-Inferenz optimiert")
                logger.info("  - CuDNN Frontend deaktiviert (f√ºr Kompatibilit√§t)")
        
        # Aktuelle GPU
        current_device = torch.cuda.current_device()
        logger.info(f"Aktuelle GPU: {current_device}")
        
        # Memory Info
        allocated = torch.cuda.memory_allocated(0) / 1024**2
        reserved = torch.cuda.memory_reserved(0) / 1024**2
        logger.info(f"GPU Memory - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB")
        
        # CUDA Version
        logger.info(f"CUDA Version: {torch.version.cuda}")
        logger.info(f"cuDNN Version: {torch.backends.cudnn.version()}")
        logger.info(f"PyTorch Version: {torch.__version__}")
        
        # ONNX Runtime Check
        try:
            import onnxruntime as ort
            logger.info(f"ONNX Runtime Version: {ort.__version__}")
            providers = ort.get_available_providers()
            logger.info(f"ONNX Providers: {providers}")
            if 'CUDAExecutionProvider' in providers:
                logger.info("‚úì ONNX Runtime GPU Support aktiviert")
            else:
                logger.warning("‚ö† ONNX Runtime ohne GPU Support")
        except ImportError:
            logger.warning("‚ö† ONNX Runtime nicht installiert")
            logger.info("Installation: pip install onnxruntime-gpu==1.17.1")
        
    else:
        logger.warning("‚ö† CUDA nicht verf√ºgbar! L√§uft auf CPU.")
        logger.warning("Bitte PyTorch mit CUDA-Support installieren:")
        logger.warning("pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu124")
    
    logger.info("=" * 70)
    
    return cuda_available


def is_tesla_p4() -> bool:
    """Erkennt ob Tesla P4 GPU vorhanden ist"""
    if not torch.cuda.is_available():
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    return "Tesla P4" in gpu_name or "P4" in gpu_name


class FaceCropper:
    """
    Speichert erkannte Gesichter als separate Bild-Ausschnitte
    NEU: Nur Gesichter mit bekannten Namen (gelbe Boxen)
    """
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"FaceCropper initialisiert - Output: {self.output_dir}")
        
        self.saved_count = 0
    
    def save_face_crop(self, image: np.ndarray, face: Dict[str, Any], 
                      source_filename: str, recording_id: int) -> Optional[Path]:
        """
        Speichert Gesichts-Ausschnitt nur wenn Person bekannt ist
        
        Args:
            image: Original-Bild
            face: Face-Detection mit bbox und name
            source_filename: Original-Dateiname
            recording_id: Recording ID aus DB
            
        Returns:
            Pfad zum gespeicherten Crop oder None
        """
        # NUR bekannte Gesichter speichern
        if face.get('name') == 'Unknown':
            return None
        
        try:
            bbox = face['bbox']
            x1 = max(0, int(bbox['x1']))
            y1 = max(0, int(bbox['y1']))
            x2 = min(image.shape[1], int(bbox['x2']))
            y2 = min(image.shape[0], int(bbox['y2']))
            
            # Sicherheitspr√ºfung
            if x2 <= x1 or y2 <= y1:
                logger.warning(f"Ung√ºltige Gesichts-Bbox: {bbox}")
                return None
            
            # Gesicht ausschneiden mit etwas Padding (10%)
            padding_x = int((x2 - x1) * 0.1)
            padding_y = int((y2 - y1) * 0.1)
            
            x1_padded = max(0, x1 - padding_x)
            y1_padded = max(0, y1 - padding_y)
            x2_padded = min(image.shape[1], x2 + padding_x)
            y2_padded = min(image.shape[0], y2 + padding_y)
            
            face_crop = image[y1_padded:y2_padded, x1_padded:x2_padded]
            
            if face_crop.size == 0:
                logger.warning(f"Leerer Gesichts-Crop")
                return None
            
            # Dateiname generieren
            person_name = face['name'].replace(' ', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            confidence = face.get('confidence', 0.0)
            
            filename = f"{person_name}_{timestamp}_conf{confidence:.2f}_rec{recording_id}.jpg"
            output_path = self.output_dir / filename
            
            # Speichern mit hoher Qualit√§t
            cv2.imwrite(str(output_path), face_crop, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            self.saved_count += 1
            logger.info(f"‚úì Gesicht gespeichert: {filename} ({face_crop.shape[1]}x{face_crop.shape[0]}px)")
            
            return output_path
            
        except Exception as e:
            logger.error(f"Fehler beim Speichern von Gesichts-Crop: {e}")
            return None


class ImageAnnotator:
    """
    Zeichnet Bounding Boxes und Labels auf Bilder
    ANGEPASST: Gelbe Boxen f√ºr BEKANNTE Gesichter
    """
    
    # Farben (BGR Format f√ºr OpenCV)
    COLOR_VEHICLE = (0, 255, 255)   # Gelb
    COLOR_PERSON = (0, 255, 0)      # Gr√ºn
    COLOR_KNOWN_FACE = (0, 255, 255)  # Gelb f√ºr bekannte Gesichter
    COLOR_UNKNOWN_FACE = (0, 0, 255) # Rot f√ºr unbekannte Gesichter
    COLOR_OBJECT = (255, 0, 255)    # Magenta
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ImageAnnotator initialisiert - Output: {self.output_dir}")
    
    def annotate_image(self, image_path: Path, analysis_results: Dict[str, Any], 
                      save_prefix: str = "annotated") -> Optional[Path]:
        """
        Zeichnet alle Detektionen auf das Bild
        GELBE BOXEN nur f√ºr BEKANNTE Gesichter
        """
        try:
            # Bild laden
            image = cv2.imread(str(image_path))
            if image is None:
                logger.error(f"Bild konnte nicht geladen werden: {image_path}")
                return None
            
            # Detektionen zeichnen
            detection_count = 0
            known_faces_count = 0
            
            # 1. Fahrzeuge (Gelbe Boxen)
            for vehicle in analysis_results.get('vehicles', []):
                self._draw_bbox(image, vehicle, self.COLOR_VEHICLE, vehicle['class'])
                detection_count += 1
            
            # 2. Personen (Gr√ºne Boxen)
            for obj in analysis_results.get('objects', []):
                if obj['class'] == 'person':
                    self._draw_bbox(image, obj, self.COLOR_PERSON, 'Person')
                    detection_count += 1
            
            # 3. Gesichter - GELB f√ºr bekannt, ROT f√ºr unbekannt
            for face in analysis_results.get('faces', []):
                if face['name'] != 'Unknown':
                    # BEKANNTE Gesichter = GELBE BOX
                    self._draw_face_bbox(image, face, self.COLOR_KNOWN_FACE)
                    detection_count += 1
                    known_faces_count += 1
                else:
                    # Unbekannte Gesichter = Rote Box
                    self._draw_face_bbox(image, face, self.COLOR_UNKNOWN_FACE)
                    detection_count += 1
            
            # 4. Andere interessante Objekte (Magenta)
            interesting_objects = ['dog', 'cat', 'bird', 'horse', 'sheep', 'cow', 
                                 'elephant', 'bear', 'zebra', 'giraffe']
            for obj in analysis_results.get('objects', []):
                if obj['class'] in interesting_objects:
                    self._draw_bbox(image, obj, self.COLOR_OBJECT, obj['class'])
                    detection_count += 1
            
            # Nur speichern wenn Detektionen vorhanden
            if detection_count == 0:
                logger.debug(f"Keine Detektionen in {image_path.name} - √ºberspringe Annotation")
                return None
            
            # Info-Text oben links
            info_text = f"Detektionen: {detection_count} | Bekannte Gesichter: {known_faces_count} | GPU: {analysis_results.get('gpu_used', False)}"
            cv2.putText(image, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(image, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)
            
            # Legende
            legend_y = 60
            cv2.putText(image, "Gelb = Bekannte Person/Fahrzeug", (10, legend_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
            cv2.putText(image, "Gruen = Person", (10, legend_y + 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # Ausgabepfad erstellen
            output_filename = f"{save_prefix}_{image_path.name}"
            output_path = self.output_dir / output_filename
            
            # Speichern
            cv2.imwrite(str(output_path), image)
            logger.debug(f"‚úì Annotiert ({detection_count} Det., {known_faces_count} bekannte): {output_filename}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"Fehler beim Annotieren von {image_path}: {e}")
            return None
    
    def _draw_bbox(self, image: np.ndarray, detection: Dict[str, Any], 
                   color: Tuple[int, int, int], label_text: str):
        """Zeichnet Bounding Box mit Label"""
        bbox = detection['bbox']
        x1 = int(bbox['x1'])
        y1 = int(bbox['y1'])
        x2 = int(bbox['x2'])
        y2 = int(bbox['y2'])
        conf = detection.get('confidence', 0.0)
        
        # Box zeichnen (dickere Linie)
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 3)
        
        # Label mit Hintergrund
        label = f"{label_text} {conf:.2f}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        
        # Hintergrund f√ºr bessere Lesbarkeit
        cv2.rectangle(image, 
                     (x1, y1 - label_size[1] - 10), 
                     (x1 + label_size[0], y1), 
                     color, -1)
        
        # Text
        cv2.putText(image, label, (x1, y1 - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
    
    def _draw_face_bbox(self, image: np.ndarray, face: Dict[str, Any], 
                       color: Tuple[int, int, int]):
        """Zeichnet Gesichts-Bounding Box mit Namen"""
        bbox = face['bbox']
        x1 = bbox['x1']
        y1 = bbox['y1']
        x2 = bbox['x2']
        y2 = bbox['y2']
        conf = face.get('confidence', 0.0)
        name = face.get('name', 'Unknown')
        
        # Box zeichnen (dicker f√ºr bekannte Gesichter)
        thickness = 4 if name != 'Unknown' else 2
        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)
        
        # Label mit Hintergrund
        label = f"{name} {conf:.2f}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
        
        cv2.rectangle(image, 
                     (x1, y1 - label_size[1] - 10), 
                     (x1 + label_size[0], y1), 
                     color, -1)
        
        cv2.putText(image, label, (x1, y1 - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)


class AIAnalyzer:
    """AI-Analyse Engine f√ºr Video und Bilder - Optimiert f√ºr Tesla P4 mit InsightFace GPU"""
    
    def __init__(self, yolo_model_path: str, known_faces_dir: str, force_gpu: bool = True):
        self.yolo_model = None
        self.known_faces_dir = Path(known_faces_dir)
        self.known_face_encodings = []
        self.known_face_names = []
        self.force_gpu = force_gpu
        self.is_tesla_p4 = False
        self.face_app = None  # InsightFace App
        
        # GPU-Device bestimmen
        if torch.cuda.is_available() and force_gpu:
            self.device = 'cuda'
            self.gpu_id = 0
            torch.cuda.set_device(self.gpu_id)
            
            # Tesla P4 Erkennung
            self.is_tesla_p4 = is_tesla_p4()
            if self.is_tesla_p4:
                logger.info("‚úì Tesla P4 erkannt - Performance-Optimierungen aktiviert")
        else:
            self.device = 'cpu'
            self.gpu_id = None
            if force_gpu:
                logger.error("GPU erzwungen, aber CUDA nicht verf√ºgbar!")
                sys.exit(1)
        
        logger.info(f"AI-Analyzer initialisiert - Device: {self.device}")
        
        # YOLO Model laden
        self._load_yolo_model(yolo_model_path)
        
        # InsightFace GPU initialisieren
        if self.device == 'cuda':
            self._init_insightface_gpu()
        
        # Bekannte Gesichter laden
        self._load_known_faces()
        
        # GPU Warmup
        if self.device == 'cuda':
            self._gpu_warmup()
    
    def _init_insightface_gpu(self):
        """
        Initialisiert InsightFace mit GPU-Support f√ºr Tesla P4
        Environment-Variablen m√ºssen bereits gesetzt sein!
        """
        try:
            from insightface.app import FaceAnalysis
            
            logger.info("Initialisiere InsightFace mit GPU-Support...")
            
            # Tesla P4 optimierte CUDA-Optionen
            cuda_options = {
                'device_id': 0,
                'cudnn_conv_algo_search': 'DEFAULT',
                'gpu_mem_limit': 6 * 1024 * 1024 * 1024,  # 6GB f√ºr P4
                'arena_extend_strategy': 'kSameAsRequested',
            }
            
            self.face_app = FaceAnalysis(
                name='buffalo_l',
                providers=[('CUDAExecutionProvider', cuda_options), 'CPUExecutionProvider']
            )
            
            # det_size f√ºr Tesla P4 optimiert (1280x1280 f√ºr 4K-Bilder)
            self.face_app.prepare(ctx_id=0, det_size=(1280, 1280))
            
            # Provider-Check
            providers = self.face_app.det_model.session.get_providers()
            logger.info(f"‚úì InsightFace GPU initialisiert")
            logger.info(f"  Providers: {providers}")
            
            if 'CUDAExecutionProvider' in providers:
                logger.info("  ‚úì GPU-Acceleration AKTIV")
            else:
                logger.warning("  ‚ö† GPU-Acceleration NICHT aktiv - l√§uft auf CPU!")
            
        except Exception as e:
            logger.error(f"InsightFace GPU-Init fehlgeschlagen: {e}")
            logger.warning("Fallback: InsightFace wird nicht verwendet")
            self.face_app = None
    
    def _load_yolo_model(self, model_path: str):
        """L√§dt YOLO Model explizit auf GPU mit Tesla P4 Optimierungen"""
        try:
            logger.info(f"Lade YOLO Model: {model_path}")
            self.yolo_model = YOLO(model_path)
            
            # Explizit auf GPU verschieben
            if self.device == 'cuda':
                self.yolo_model.to(self.device)
                logger.info(f"‚úì YOLO Model auf GPU {self.gpu_id} geladen")
                
                if self.is_tesla_p4:
                    logger.info("Tesla P4 YOLO-Optimierungen:")
                    logger.info("  - FP16 (Half Precision) aktiviert")
                    logger.info("  - Optimale Batch-Gr√∂√üe: 4-8")
                
                logger.info(f"Model-Device: {next(self.yolo_model.model.parameters()).device}")
            else:
                logger.info("YOLO Model auf CPU geladen")
                
        except Exception as e:
            logger.error(f"YOLO Model konnte nicht geladen werden: {e}")
            raise
    
    def _gpu_warmup(self):
        """Warmup f√ºr GPU - Tesla P4 optimiert"""
        logger.info("GPU Warmup (Tesla P4 optimiert)...")
        try:
            # Dummy-Bild f√ºr Warmup
            dummy_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            
            # YOLO Warmup
            warmup_iterations = 3 if self.is_tesla_p4 else 1
            
            for i in range(warmup_iterations):
                _ = self.yolo_model(
                    dummy_image, 
                    verbose=False, 
                    device=self.device,
                    half=True  # FP16 f√ºr Tesla P4
                )
            
            # InsightFace Warmup
            if self.face_app is not None:
                for i in range(2):
                    _ = self.face_app.get(dummy_image)
            
            logger.info(f"‚úì GPU Warmup abgeschlossen ({warmup_iterations} YOLO + 2 InsightFace Iterationen)")
            
            # Memory nach Warmup
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**2
                logger.info(f"GPU Memory nach Warmup: {allocated:.2f} MB")
                
        except Exception as e:
            logger.error(f"GPU Warmup fehlgeschlagen: {e}")
    
    def _load_known_faces(self):
        """L√§dt bekannte Gesichter - InsightFace GPU-basiert"""
        if not self.known_faces_dir.exists():
            logger.warning(f"Verzeichnis f√ºr bekannte Gesichter nicht gefunden: {self.known_faces_dir}")
            logger.warning(f"Erstelle Verzeichnis: {self.known_faces_dir}")
            self.known_faces_dir.mkdir(parents=True, exist_ok=True)
            return
        
        face_files = list(self.known_faces_dir.glob("*.jpg")) + list(self.known_faces_dir.glob("*.png"))
        
        if not face_files:
            logger.warning(f"Keine Gesichts-Bilder in {self.known_faces_dir} gefunden")
            logger.info("Um Gesichtserkennung zu aktivieren, legen Sie .jpg/.png Dateien in diesem Verzeichnis ab")
            logger.info("Dateiname = Name der Person (z.B. 'Max_Mustermann.jpg')")
            return
        
        logger.info(f"Lade {len(face_files)} bekannte Gesichter...")
        
        # InsightFace GPU-basiert laden (wenn verf√ºgbar)
        if self.face_app is not None:
            for image_path in face_files:
                try:
                    img = cv2.imread(str(image_path))
                    if img is None:
                        logger.warning(f"  ‚ö† Bild konnte nicht geladen werden: {image_path.name}")
                        continue
                    
                    faces = self.face_app.get(img)
                    
                    if faces:
                        # Erstes Gesicht nehmen und Embedding speichern
                        self.known_face_encodings.append(faces[0].embedding)
                        name = image_path.stem.replace('_', ' ')
                        self.known_face_names.append(name)
                        logger.info(f"  ‚úì {name} (GPU)")
                    else:
                        logger.warning(f"  ‚ö† Kein Gesicht gefunden: {image_path.name}")
                except Exception as e:
                    logger.error(f"  ‚úó Fehler bei {image_path.name}: {e}")
        else:
            logger.warning("InsightFace nicht verf√ºgbar - Gesichtserkennung deaktiviert")
            return
        
        logger.info(f"‚úì {len(self.known_face_names)} bekannte Gesichter geladen (GPU-basiert)")
    
    def get_gpu_stats(self) -> Dict[str, Any]:
        """Gibt aktuelle GPU-Statistiken zur√ºck"""
        if not torch.cuda.is_available():
            return {'available': False}
        
        return {
            'available': True,
            'device': self.gpu_id,
            'name': torch.cuda.get_device_name(self.gpu_id),
            'is_tesla_p4': self.is_tesla_p4,
            'memory_allocated_mb': torch.cuda.memory_allocated(self.gpu_id) / 1024**2,
            'memory_reserved_mb': torch.cuda.memory_reserved(self.gpu_id) / 1024**2,
            'memory_total_gb': torch.cuda.get_device_properties(self.gpu_id).total_memory / 1024**3
        }
    
    def analyze_image(self, image_path: Path) -> Dict[str, Any]:
        """Analysiert ein einzelnes Bild"""
        results = {
            'faces': [],
            'objects': [],
            'vehicles': [],
            'persons': 0,
            'known_faces_count': 0,
            'analysis_timestamp': datetime.now(),
            'gpu_used': self.device == 'cuda'
        }
        
        try:
            # Bild laden
            image = cv2.imread(str(image_path))
            if image is None:
                logger.error(f"Bild konnte nicht geladen werden: {image_path}")
                return results
            
            # Gesichtserkennung (GPU mit InsightFace)
            face_results = self._detect_faces(image)
            results['faces'] = face_results
            
            # Z√§hle bekannte Gesichter
            results['known_faces_count'] = sum(1 for f in face_results if f['name'] != 'Unknown')
            
            # Objekt-Detektion mit YOLO (GPU)
            yolo_results = self._detect_objects(image)
            results['objects'] = yolo_results['objects']
            results['vehicles'] = yolo_results['vehicles']
            results['persons'] = yolo_results['persons']

            # Szenen-Klassifikation
            results['scene_category'] = self._classify_scene(results)

        except Exception as e:
            logger.error(f"Fehler bei Bildanalyse {image_path}: {e}")

        return results
    
    def _detect_faces(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        Erkennt Gesichter mit InsightFace (GPU-beschleunigt)
        """
        faces = []
        
        if self.face_app is None:
            logger.debug("InsightFace nicht verf√ºgbar - keine Gesichtserkennung")
            return faces
        
        try:
            # InsightFace Detection (GPU)
            detected_faces = self.face_app.get(image)
            
            for face in detected_faces:
                # Face Recognition mit bekannten Gesichtern vergleichen
                name = "Unknown"
                confidence = 0.0
                
                if self.known_face_encodings:
                    # InsightFace Embedding
                    face_embedding = face.embedding
                    
                    # Cosine Similarity mit allen bekannten Gesichtern
                    similarities = []
                    for known_embedding in self.known_face_encodings:
                        # Cosine Similarity berechnen
                        similarity = np.dot(face_embedding, known_embedding) / (
                            np.linalg.norm(face_embedding) * np.linalg.norm(known_embedding)
                        )
                        similarities.append(similarity)
                    
                    # Bestes Match finden
                    if similarities:
                        best_idx = np.argmax(similarities)
                        best_similarity = similarities[best_idx]
                        
                        # Threshold f√ºr Match (0.4 = 40% √Ñhnlichkeit f√ºr InsightFace)
                        # InsightFace ist genauer, daher niedrigerer Threshold als face_recognition
                        if best_similarity > 0.4:
                            name = self.known_face_names[best_idx]
                            confidence = float(best_similarity)
                
                # Bounding Box
                bbox = face.bbox.astype(int)
                
                faces.append({
                    'name': name,
                    'confidence': float(confidence) if name != "Unknown" else 0.0,
                    'bbox': {
                        'x1': int(bbox[0]),
                        'y1': int(bbox[1]),
                        'x2': int(bbox[2]),
                        'y2': int(bbox[3])
                    }
                })
                
                if name != "Unknown":
                    logger.debug(f"‚úì Bekanntes Gesicht (GPU): {name} ({confidence:.2f})")
        
        except Exception as e:
            logger.error(f"Fehler bei InsightFace GPU-Detection: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return faces

    def _calculate_parking_spot_id(self, bbox: Dict[str, float], image_width: int = 4512, image_height: int = 2512,
                                   grid_cols: int = 4, grid_rows: int = 3) -> int:
        """
        Berechnet parking_spot_id basierend auf Fahrzeug-Position im Grid

        Args:
            bbox: Bounding Box mit x1, y1, x2, y2
            image_width: Bildbreite (Standard: 4K Reolink)
            image_height: Bildh√∂he
            grid_cols: Anzahl Grid-Spalten (Standard: 4)
            grid_rows: Anzahl Grid-Zeilen (Standard: 3)

        Returns:
            parking_spot_id: 1-12 (bei 4x3 Grid)
        """
        # Mittelpunkt des Fahrzeugs berechnen
        center_x = (bbox['x1'] + bbox['x2']) / 2
        center_y = (bbox['y1'] + bbox['y2']) / 2

        # Grid-Cell berechnen
        cell_width = image_width / grid_cols
        cell_height = image_height / grid_rows

        col = min(int(center_x / cell_width), grid_cols - 1)
        row = min(int(center_y / cell_height), grid_rows - 1)

        # parking_spot_id: 1-basiert, von links nach rechts, oben nach unten
        # Row 0: IDs 1-4, Row 1: IDs 5-8, Row 2: IDs 9-12
        parking_spot_id = (row * grid_cols) + col + 1

        return parking_spot_id

    def _detect_objects(self, image: np.ndarray) -> Dict[str, Any]:
        """Erkennt Objekte mit YOLO - Tesla P4 optimiert"""
        results = {
            'objects': [],
            'vehicles': [],
            'persons': 0
        }
        
        if self.yolo_model is None:
            return results
        
        try:
            # YOLO Inference mit Tesla P4 Optimierungen
            detections = self.yolo_model(
                image, 
                verbose=False,
                device=self.device,
                half=True if (self.device == 'cuda' and self.is_tesla_p4) else False,  # FP16 f√ºr P4
                conf=0.25,  # Confidence Threshold
                iou=0.45    # NMS IOU Threshold
            )[0]
            
            vehicle_classes = ['car', 'truck', 'bus', 'motorcycle', 'bicycle']
            
            for detection in detections.boxes.data:
                x1, y1, x2, y2, conf, cls = detection
                class_id = int(cls)
                class_name = self.yolo_model.names[class_id]
                confidence = float(conf)
                
                obj_data = {
                    'class': class_name,
                    'confidence': confidence,
                    'bbox': {
                        'x1': float(x1),
                        'y1': float(y1),
                        'x2': float(x2),
                        'y2': float(y2)
                    }
                }
                
                results['objects'].append(obj_data)
                
                # Spezielle Kategorien
                if class_name == 'person':
                    results['persons'] += 1
                elif class_name in vehicle_classes:
                    # Berechne parking_spot_id f√ºr Fahrzeuge
                    image_height, image_width = image.shape[:2]
                    parking_spot_id = self._calculate_parking_spot_id(
                        obj_data['bbox'],
                        image_width,
                        image_height
                    )
                    obj_data['parking_spot_id'] = parking_spot_id
                    results['vehicles'].append(obj_data)
                    logger.debug(f"Fahrzeug erkannt: {class_name} (Konfidenz: {confidence:.2f}, Parkplatz: {parking_spot_id})")
                else:
                    logger.debug(f"Objekt erkannt: {class_name} (Konfidenz: {confidence:.2f})")
        
        except Exception as e:
            logger.error(f"Fehler bei YOLO-Detektion: {e}")
        
        return results
    
    def analyze_video(self, video_path: Path, sample_rate: int = 30) -> Dict[str, Any]:
        """
        Analysiert Video durch Sampling von Frames - Tesla P4 optimiert
        """
        results = {
            'faces': [],
            'objects': [],
            'vehicles': [],
            'max_persons': 0,
            'total_frames': 0,
            'analyzed_frames': 0,
            'known_faces_count': 0,
            'analysis_timestamp': datetime.now(),
            'gpu_used': self.device == 'cuda'
        }
        
        try:
            cap = cv2.VideoCapture(str(video_path))
            
            if not cap.isOpened():
                logger.error(f"Video konnte nicht ge√∂ffnet werden: {video_path}")
                return results
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            results['total_frames'] = total_frames
            
            frame_count = 0
            analyzed_count = 0
            
            # Sets f√ºr eindeutige Detektionen
            unique_faces = set()
            unique_objects = set()
            unique_vehicles = set()
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # Nur jeden N-ten Frame analysieren
                if frame_count % sample_rate != 0:
                    continue
                
                analyzed_count += 1
                
                # Frame analysieren
                frame_results = self.analyze_image_array(frame)
                
                # Gesichter sammeln
                for face in frame_results['faces']:
                    if face['name'] != "Unknown":
                        unique_faces.add(face['name'])
                
                # Objekte sammeln
                for obj in frame_results['objects']:
                    unique_objects.add(obj['class'])
                
                # Fahrzeuge sammeln
                for vehicle in frame_results['vehicles']:
                    unique_vehicles.add(vehicle['class'])
                
                # Max Personen tracken
                if frame_results['persons'] > results['max_persons']:
                    results['max_persons'] = frame_results['persons']
                
                if analyzed_count % 10 == 0:
                    logger.debug(f"Video-Analyse: {analyzed_count} Frames analysiert")
            
            cap.release()
            
            results['analyzed_frames'] = analyzed_count
            results['faces'] = [{'name': name} for name in unique_faces]
            results['known_faces_count'] = len(unique_faces)
            results['objects'] = [{'class': obj} for obj in unique_objects]
            results['vehicles'] = [{'class': veh} for veh in unique_vehicles]

            # Szenen-Klassifikation basierend auf Video-Analyse
            results['scene_category'] = self._classify_scene(results)

            logger.info(f"Video analysiert: {analyzed_count}/{total_frames} Frames, "
                       f"{len(unique_faces)} bekannte Gesichter, {results['max_persons']} max. Personen, "
                       f"Szene: {results['scene_category']}")

        except Exception as e:
            logger.error(f"Fehler bei Video-Analyse {video_path}: {e}")

        return results

    def _classify_scene(self, results: Dict[str, Any]) -> str:
        """
        Klassifiziert die Szene basierend auf erkannten Objekten
        Kategorien: parking, street, entrance, indoor, empty
        """
        vehicles = results.get('vehicles', [])
        persons = results.get('persons', 0)
        objects = results.get('objects', [])

        # Z√§hle Objekt-Typen
        num_vehicles = len(vehicles)
        num_persons = persons

        # Klassifikationslogik
        if num_vehicles >= 2 and num_persons == 0:
            return 'parking'  # Mehrere Fahrzeuge, keine Personen = Parkplatz
        elif num_vehicles >= 1 and num_persons >= 1:
            return 'street'  # Fahrzeuge + Personen = Stra√üenszene
        elif num_persons >= 2 and num_vehicles == 0:
            return 'entrance'  # Mehrere Personen ohne Fahrzeuge = Eingangsbereich
        elif num_persons == 1 and num_vehicles == 0:
            return 'visitor'  # Eine Person = Besucher
        elif num_vehicles == 1 and num_persons == 0:
            return 'delivery'  # Ein Fahrzeug alleine = Lieferung/Anlieferung
        elif len(objects) == 0:
            return 'empty'  # Keine Objekte = leere Szene
        else:
            return 'unknown'  # Alles andere

    def analyze_image_array(self, image: np.ndarray) -> Dict[str, Any]:
        """Analysiert Bild als numpy array (f√ºr Video-Frames)"""
        results = {
            'faces': [],
            'objects': [],
            'vehicles': [],
            'persons': 0,
            'known_faces_count': 0
        }
        
        try:
            # Gesichtserkennung
            face_results = self._detect_faces(image)
            results['faces'] = face_results
            results['known_faces_count'] = sum(1 for f in face_results if f['name'] != 'Unknown')
            
            # Objekt-Detektion
            yolo_results = self._detect_objects(image)
            results['objects'] = yolo_results['objects']
            results['vehicles'] = yolo_results['vehicles']
            results['persons'] = yolo_results['persons']

            # Szenen-Klassifikation
            results['scene_category'] = self._classify_scene(results)

        except Exception as e:
            logger.error(f"Fehler bei Array-Analyse: {e}")
        
        return results


class FileProcessor:
    """Verarbeitet Mediendateien mit AI-Analyse und Datenbank-Integration"""
    
    def __init__(self, base_path: str, db_config: dict, ai_analyzer: AIAnalyzer, 
                 annotator: Optional[ImageAnnotator] = None,
                 face_cropper: Optional[FaceCropper] = None,
                 reanalyze: bool = False):  # NEU
        self.base_path = Path(base_path)
        self.db_config = db_config
        self.db_connection = None
        self.ai_analyzer = ai_analyzer
        self.annotator = annotator
        self.face_cropper = face_cropper
        self.reanalyze = reanalyze  # NEU
        
        # Statistiken
        self.processed_count = 0
        self.skipped_count = 0
        self.error_count = 0
        self.analyzed_count = 0
        self.annotated_count = 0
        self.face_crops_saved = 0
        self.total_analysis_time = 0.0
        
    def connect_db(self) -> bool:
        """Stellt Datenbankverbindung her"""
        try:
            self.db_connection = pymysql.connect(**self.db_config)
            logger.info("Datenbankverbindung erfolgreich hergestellt")
            return True
        except Exception as e:
            logger.error(f"Datenbankverbindung fehlgeschlagen: {e}")
            return False
    
    def disconnect_db(self):
        """Schlie√üt Datenbankverbindung"""
        if self.db_connection:
            self.db_connection.close()
            logger.info("Datenbankverbindung geschlossen")
    
    def parse_filename(self, filename: str) -> Optional[Tuple[str, str, datetime]]:
        """Parst Dateinamen nach dem Muster: Camera1_00_20260121074033.jpg/mp4"""
        pattern = r'^(Camera\d+)_(\d{2})_(\d{14})\.(jpg|mp4)$'
        match = re.match(pattern, filename)
        
        if not match:
            return None
        
        camera_name = match.group(1)
        timestamp_str = match.group(3)
        file_extension = match.group(4)
        
        try:
            timestamp = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S')
            return (camera_name, file_extension, timestamp)
        except ValueError as e:
            logger.error(f"Zeitstempel-Parsing fehlgeschlagen f√ºr {filename}: {e}")
            return None
    
    def file_exists_in_db(self, filepath: str) -> bool:
        """Pr√ºft ob Datei bereits in DB existiert"""
        try:
            cursor = self.db_connection.cursor()
            query = "SELECT COUNT(*) FROM cam2_recordings WHERE file_path = %s"
            cursor.execute(query, (filepath,))
            count = cursor.fetchone()[0]
            cursor.close()
            return count > 0
        except Exception as e:
            logger.error(f"DB-Abfrage fehlgeschlagen: {e}")
            return False
    
    def _delete_file_from_db(self, filepath: str):
        """L√∂scht Datei und alle zugeh√∂rigen Analysen aus DB (f√ºr --reanalyze)"""
        try:
            cursor = self.db_connection.cursor()
            
            # Hole recording_id
            query = "SELECT id FROM cam2_recordings WHERE file_path = %s"
            cursor.execute(query, (filepath,))
            result = cursor.fetchone()
            
            if result:
                recording_id = result[0]
                
                # L√∂sche Analysen (CASCADE l√∂scht automatisch faces, objects, summary)
                query = "DELETE FROM cam2_recordings WHERE id = %s"
                cursor.execute(query, (recording_id,))
                
                self.db_connection.commit()
                logger.debug(f"‚úì Alte DB-Eintr√§ge gel√∂scht: {filepath}")
            
            cursor.close()
            
        except Exception as e:
            logger.error(f"Fehler beim L√∂schen von DB-Eintr√§gen f√ºr {filepath}: {e}")
            self.db_connection.rollback()
    
    def insert_file_to_db(self, filepath: Path, camera_name: str, 
                         file_type: str, timestamp: datetime,
                         analysis_results: Optional[Dict[str, Any]] = None) -> Optional[int]:
        """Tr√§gt Datei mit AI-Analyse in die Datenbank ein"""
        try:
            cursor = self.db_connection.cursor()
            
            # Relativer Pfad zur Basis
            rel_path = str(filepath.relative_to(self.base_path))
            
            # Dateigr√∂√üe ermitteln
            file_size = filepath.stat().st_size
            
            # Haupt-Recording eintragen
            query = """
                INSERT INTO cam2_recordings 
                (camera_name, file_path, file_type, file_size, recorded_at, 
                 analyzed, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, NOW())
            """
            
            values = (
                camera_name,
                rel_path,
                file_type,
                file_size,
                timestamp,
                analysis_results is not None
            )
            
            cursor.execute(query, values)
            recording_id = cursor.lastrowid
            
            # AI-Analyse Ergebnisse eintragen
            if analysis_results:
                self._insert_analysis_results(cursor, recording_id, analysis_results)
            
            self.db_connection.commit()
            cursor.close()
            
            logger.info(f"‚úì Eingetragen: {rel_path} (ID: {recording_id}, {file_size/1024/1024:.2f} MB)")
            return recording_id
            
        except Exception as e:
            logger.error(f"DB-Insert fehlgeschlagen f√ºr {filepath}: {e}")
            self.db_connection.rollback()
            return None
    
    def _insert_analysis_results(self, cursor, recording_id: int, results: Dict[str, Any]):
        """Tr√§gt AI-Analyse Ergebnisse in DB ein"""
        try:
            # Gesichter eintragen
            for face in results.get('faces', []):
                query = """
                    INSERT INTO cam2_detected_faces 
                    (recording_id, person_name, confidence, bbox_x1, bbox_y1, bbox_x2, bbox_y2)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                """
                bbox = face.get('bbox', {})
                values = (
                    recording_id,
                    face.get('name', 'Unknown'),
                    face.get('confidence', 0.0),
                    bbox.get('x1', 0),
                    bbox.get('y1', 0),
                    bbox.get('x2', 0),
                    bbox.get('y2', 0)
                )
                cursor.execute(query, values)
            
            # Objekte eintragen
            for obj in results.get('objects', []):
                query = """
                    INSERT INTO cam2_detected_objects
                    (recording_id, object_class, confidence, bbox_x1, bbox_y1, bbox_x2, bbox_y2, parking_spot_id)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """
                bbox = obj.get('bbox', {})
                values = (
                    recording_id,
                    obj.get('class', 'unknown'),
                    obj.get('confidence', 0.0),
                    bbox.get('x1', 0),
                    bbox.get('y1', 0),
                    bbox.get('x2', 0),
                    bbox.get('y2', 0),
                    obj.get('parking_spot_id', None)  # NULL f√ºr nicht-Fahrzeuge
                )
                cursor.execute(query, values)
            
            # Zusammenfassung eintragen
            query = """
                INSERT INTO cam2_analysis_summary
                (recording_id, total_faces, total_objects, total_vehicles,
                 max_persons, scene_category, gpu_used, analyzed_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
            """
            values = (
                recording_id,
                len(results.get('faces', [])),
                len(results.get('objects', [])),
                len(results.get('vehicles', [])),
                results.get('max_persons', results.get('persons', 0)),
                results.get('scene_category', 'unknown'),
                results.get('gpu_used', False)
            )
            cursor.execute(query, values)
            
        except Exception as e:
            logger.error(f"Fehler beim Eintragen der Analyse-Ergebnisse: {e}")
            raise
    
    def find_all_media_files(self) -> List[Path]:
        """Findet alle Mediendateien rekursiv"""
        media_files = []
        
        for ext in ['*.mp4', '*.jpg']:
            media_files.extend(self.base_path.rglob(ext))
        
        # Nach Datum sortieren (neueste zuerst)
        media_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        
        logger.info(f"Gefunden: {len(media_files)} Mediendateien")
        return media_files
    
    def process_file(self, filepath: Path, analyze: bool = True) -> bool:
        """Verarbeitet eine einzelne Datei mit optionaler AI-Analyse"""
        filename = filepath.name
        
        # Dateinamen parsen
        parsed = self.parse_filename(filename)
        if not parsed:
            logger.warning(f"‚ö† √úbersprungen (ung√ºltiges Format): {filename}")
            self.skipped_count += 1
            return False
        
        camera_name, file_type, timestamp = parsed
        
        # Pr√ºfen ob bereits in DB
        rel_path = str(filepath.relative_to(self.base_path))
        
        # NEU: Bei --reanalyze alte Eintr√§ge l√∂schen
        if self.reanalyze and self.file_exists_in_db(rel_path):
            self._delete_file_from_db(rel_path)
            logger.info(f"üîÑ Reanalyze: Alte Daten gel√∂scht f√ºr {filename}")
        elif self.file_exists_in_db(rel_path):
            logger.debug(f"‚äò Bereits in DB: {filename}")
            self.skipped_count += 1
            return False
        
        # AI-Analyse durchf√ºhren
        analysis_results = None
        recording_id = None
        
        if analyze:
            try:
                analysis_start = time.time()
                
                if file_type == 'jpg':
                    logger.info(f"üîç Analysiere Bild: {filename}")
                    analysis_results = self.ai_analyzer.analyze_image(filepath)
                    
                    # Zuerst in DB eintragen um recording_id zu bekommen
                    recording_id = self.insert_file_to_db(
                        filepath, camera_name, file_type, timestamp, analysis_results
                    )
                    
                    if recording_id:
                        # Face-Crops speichern (nur f√ºr bekannte Gesichter)
                        if self.face_cropper and analysis_results.get('known_faces_count', 0) > 0:
                            image = cv2.imread(str(filepath))
                            if image is not None:
                                for face in analysis_results.get('faces', []):
                                    if face['name'] != 'Unknown':
                                        crop_path = self.face_cropper.save_face_crop(
                                            image, face, filename, recording_id
                                        )
                                        if crop_path:
                                            self.face_crops_saved += 1
                        
                        # Optional: Annotiertes Bild erstellen
                        if self.annotator and analysis_results:
                            annotated_path = self.annotator.annotate_image(
                                filepath, analysis_results
                            )
                            if annotated_path:
                                self.annotated_count += 1
                    
                elif file_type == 'mp4':
                    logger.info(f"üé• Analysiere Video: {filename}")
                    analysis_results = self.ai_analyzer.analyze_video(filepath, sample_rate=1)
                    
                    # In DB eintragen
                    recording_id = self.insert_file_to_db(
                        filepath, camera_name, file_type, timestamp, analysis_results
                    )
                
                analysis_time = time.time() - analysis_start
                self.total_analysis_time += analysis_time
                self.analyzed_count += 1
                
                logger.info(f"  ‚è± Analyse-Zeit: {analysis_time:.2f}s")
                
                if recording_id:
                    self.processed_count += 1
                    return True
                
            except Exception as e:
                logger.error(f"Fehler bei AI-Analyse von {filename}: {e}")
                import traceback
                logger.debug(traceback.format_exc())
                self.error_count += 1
                return False
        else:
            # Ohne Analyse nur in DB eintragen
            recording_id = self.insert_file_to_db(
                filepath, camera_name, file_type, timestamp, None
            )
            
            if recording_id:
                self.processed_count += 1
                return True
        
        self.error_count += 1
        return False
    
    def process_all_files(self, limit: Optional[int] = None, analyze: bool = True):
        """Verarbeitet alle gefundenen Dateien"""
        if not self.connect_db():
            logger.error("Abbruch: Keine Datenbankverbindung")
            return
        
        logger.info("=" * 70)
        logger.info("Starte Dateiverarbeitung mit AI-Analyse (Tesla P4 optimiert)")
        logger.info(f"AI-Device: {self.ai_analyzer.device}")
        logger.info(f"Bekannte Gesichter: {len(self.ai_analyzer.known_face_names)}")
        logger.info(f"Annotation: {'Aktiviert' if self.annotator else 'Deaktiviert'}")
        logger.info(f"Face-Cropping: {'Aktiviert (nur bekannte)' if self.face_cropper else 'Deaktiviert'}")
        logger.info(f"InsightFace GPU: {'Aktiviert' if self.ai_analyzer.face_app else 'Deaktiviert'}")
        logger.info(f"Reanalyze-Modus: {'Aktiviert (√ºberschreibt bestehende Analysen)' if self.reanalyze else 'Deaktiviert'}")  # NEU
        
        # GPU Stats anzeigen
        gpu_stats = self.ai_analyzer.get_gpu_stats()
        if gpu_stats['available']:
            logger.info(f"GPU: {gpu_stats['name']}")
            if gpu_stats.get('is_tesla_p4'):
                logger.info("‚úì Tesla P4 erkannt - optimale Performance-Einstellungen aktiv")
            logger.info(f"GPU Memory: {gpu_stats['memory_allocated_mb']:.2f} MB / {gpu_stats['memory_total_gb']:.2f} GB")
        
        logger.info("=" * 70)
        
        media_files = self.find_all_media_files()
        
        if limit:
            media_files = media_files[:limit]
            logger.info(f"Limitierung aktiv: Verarbeite nur erste {limit} Dateien")
        
        start_time = time.time()
        
        for idx, filepath in enumerate(media_files, 1):
            # Fortschritt alle 50 Dateien
            if idx % 50 == 0:
                elapsed = time.time() - start_time
                rate = idx / elapsed if elapsed > 0 else 0
                logger.info(f"Fortschritt: {idx}/{len(media_files)} Dateien "
                          f"({rate:.2f} Dateien/Sek)")
                
                # GPU Stats alle 50 Dateien
                if gpu_stats['available']:
                    gpu_stats = self.ai_analyzer.get_gpu_stats()
                    logger.info(f"GPU Memory: {gpu_stats['memory_allocated_mb']:.2f} MB")
            
            self.process_file(filepath, analyze=analyze)
        
        elapsed = time.time() - start_time
        
        logger.info("=" * 70)
        logger.info("Verarbeitung abgeschlossen")
        logger.info(f"Verarbeitet: {self.processed_count} Dateien")
        logger.info(f"AI-Analysiert: {self.analyzed_count} Dateien")
        logger.info(f"Annotiert: {self.annotated_count} Bilder")
        logger.info(f"Gesichts-Crops: {self.face_crops_saved} (nur bekannte Personen)")
        logger.info(f"√úbersprungen: {self.skipped_count} Dateien")
        logger.info(f"Fehler: {self.error_count} Dateien")
        logger.info(f"Gesamt-Dauer: {elapsed:.2f} Sekunden ({elapsed/60:.2f} Minuten)")
        if self.processed_count > 0:
            logger.info(f"Durchschnitt: {elapsed/self.processed_count:.2f} Sek/Datei")
        if self.analyzed_count > 0:
            logger.info(f"Reine Analyse-Zeit: {self.total_analysis_time:.2f} Sekunden")
            logger.info(f"Durchschnitt Analyse: {self.total_analysis_time/self.analyzed_count:.2f} Sek/Datei")
        
        # Finale GPU Stats
        if gpu_stats['available']:
            gpu_stats = self.ai_analyzer.get_gpu_stats()
            logger.info(f"Finale GPU Memory: {gpu_stats['memory_allocated_mb']:.2f} MB")
        
        logger.info("=" * 70)
        
        self.disconnect_db()


def create_database_schema():
    """Erstellt die notwendigen Datenbanktabellen"""
    schema = """
    CREATE TABLE IF NOT EXISTS cam2_recordings (
        id INT AUTO_INCREMENT PRIMARY KEY,
        camera_name VARCHAR(50) NOT NULL,
        file_path VARCHAR(255) NOT NULL UNIQUE,
        file_type ENUM('jpg', 'mp4') NOT NULL,
        file_size BIGINT NOT NULL,
        recorded_at DATETIME NOT NULL,
        analyzed BOOLEAN DEFAULT FALSE,
        created_at DATETIME NOT NULL,
        INDEX idx_camera (camera_name),
        INDEX idx_recorded (recorded_at),
        INDEX idx_analyzed (analyzed),
        INDEX idx_type (file_type)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    
    CREATE TABLE IF NOT EXISTS cam2_detected_faces (
        id INT AUTO_INCREMENT PRIMARY KEY,
        recording_id INT NOT NULL,
        person_name VARCHAR(100) NOT NULL,
        confidence FLOAT NOT NULL,
        bbox_x1 INT,
        bbox_y1 INT,
        bbox_x2 INT,
        bbox_y2 INT,
        detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (recording_id) REFERENCES cam2_recordings(id) ON DELETE CASCADE,
        INDEX idx_person (person_name),
        INDEX idx_recording (recording_id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    
    CREATE TABLE IF NOT EXISTS cam2_detected_objects (
        id INT AUTO_INCREMENT PRIMARY KEY,
        recording_id INT NOT NULL,
        object_class VARCHAR(50) NOT NULL,
        confidence FLOAT NOT NULL,
        bbox_x1 INT,
        bbox_y1 INT,
        bbox_x2 INT,
        bbox_y2 INT,
        parking_spot_id INT DEFAULT NULL,
        detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (recording_id) REFERENCES cam2_recordings(id) ON DELETE CASCADE,
        INDEX idx_class (object_class),
        INDEX idx_recording (recording_id),
        INDEX idx_parking_spot (parking_spot_id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

    CREATE TABLE IF NOT EXISTS cam2_analysis_summary (
        id INT AUTO_INCREMENT PRIMARY KEY,
        recording_id INT NOT NULL UNIQUE,
        total_faces INT DEFAULT 0,
        total_objects INT DEFAULT 0,
        total_vehicles INT DEFAULT 0,
        max_persons INT DEFAULT 0,
        scene_category VARCHAR(50) DEFAULT 'unknown',
        gpu_used BOOLEAN DEFAULT FALSE,
        analyzed_at DATETIME NOT NULL,
        FOREIGN KEY (recording_id) REFERENCES cam2_recordings(id) ON DELETE CASCADE,
        INDEX idx_recording (recording_id),
        INDEX idx_scene (scene_category)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

    CREATE TABLE IF NOT EXISTS cam2_parking_stats (
        id INT AUTO_INCREMENT PRIMARY KEY,
        parking_spot_id INT NOT NULL,
        date DATE NOT NULL,
        hour INT NOT NULL,
        occupancy_count INT DEFAULT 0,
        vehicle_type VARCHAR(50),
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
        UNIQUE KEY unique_spot_time (parking_spot_id, date, hour, vehicle_type),
        INDEX idx_spot (parking_spot_id),
        INDEX idx_date (date),
        INDEX idx_spot_date (parking_spot_id, date)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    """
    
    try:
        connection = pymysql.connect(**DB_CONFIG)
        cursor = connection.cursor()
        
        for statement in schema.split(';'):
            statement = statement.strip()
            if statement:
                cursor.execute(statement)
        
        connection.commit()
        cursor.close()
        connection.close()
        logger.info("‚úì Datenbankschema erfolgreich erstellt/√ºberpr√ºft")
        return True
    except Exception as e:
        logger.error(f"Schema-Erstellung fehlgeschlagen: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='AI-Powered Video/Image Processor - Tesla P4 Optimiert mit InsightFace GPU'
    )
    parser.add_argument(
        '--base-path',
        default=MEDIA_BASE_PATH,
        help=f'Basis-Pfad f√ºr Mediendateien (default: {MEDIA_BASE_PATH})'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='Maximale Anzahl zu verarbeitender Dateien (f√ºr Tests)'
    )
    parser.add_argument(
        '--no-analysis',
        action='store_true',
        help='Keine AI-Analyse durchf√ºhren (nur DB-Import)'
    )
    parser.add_argument(
        '--save-annotated',
        action='store_true',
        help='Speichert annotierte Bilder mit Bounding Boxes (gelb = bekannt)'
    )
    parser.add_argument(
        '--save-face-crops',
        action='store_true',
        help='Speichert Gesichts-Crops (nur bekannte Personen)'
    )
    parser.add_argument(
        '--annotated-path',
        default=ANNOTATED_OUTPUT_PATH,
        help=f'Ausgabepfad f√ºr annotierte Bilder (default: {ANNOTATED_OUTPUT_PATH})'
    )
    parser.add_argument(
        '--faces-path',
        default=FACES_OUTPUT_PATH,
        help=f'Ausgabepfad f√ºr Gesichts-Crops (default: {FACES_OUTPUT_PATH})'
    )
    parser.add_argument(
        '--cpu-only',
        action='store_true',
        help='Erzwinge CPU-Nutzung (keine GPU)'
    )
    parser.add_argument(
        '--create-schema',
        action='store_true',
        help='Erstellt Datenbanktabellen falls nicht vorhanden'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Debug-Logging aktivieren'
    )
    parser.add_argument(
        '--yolo-model',
        default=YOLO_MODEL_PATH,
        help=f'Pfad zum YOLO-Model (default: {YOLO_MODEL_PATH})'
    )
    parser.add_argument(
        '--known-faces',
        default=KNOWN_FACES_DIR,
        help=f'Verzeichnis mit bekannten Gesichtern (default: {KNOWN_FACES_DIR})'
    )
    parser.add_argument(
        '--reanalyze',
        action='store_true',
        help='Analysiert bereits analysierte Dateien erneut (√ºberschreibt DB-Eintr√§ge)'
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # GPU-Check durchf√ºhren (mit Tesla P4 Erkennung)
    gpu_available = check_gpu_availability()
    
    if not gpu_available and not args.cpu_only:
        logger.error("Keine GPU verf√ºgbar! Verwende --cpu-only f√ºr CPU-Modus")
        logger.error("Oder installiere PyTorch mit CUDA:")
        logger.error("  pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu124")
        sys.exit(1)
    
    if args.create_schema:
        logger.info("Erstelle Datenbankschema...")
        if not create_database_schema():
            sys.exit(1)
        logger.info("Schema erfolgreich erstellt")
    
    # AI Analyzer initialisieren
    logger.info("Initialisiere AI-Analyzer (Tesla P4 + InsightFace GPU)...")
    ai_analyzer = AIAnalyzer(
        args.yolo_model, 
        args.known_faces,
        force_gpu=not args.cpu_only
    )
    
    # Optional: Annotator initialisieren
    annotator = None
    if args.save_annotated:
        logger.info("Annotation aktiviert (gelbe Boxen = bekannte Personen)...")
        annotator = ImageAnnotator(args.annotated_path)
    
    # Optional: Face-Cropper initialisieren
    face_cropper = None
    if args.save_face_crops:
        logger.info("Face-Cropping aktiviert (nur bekannte Personen)...")
        face_cropper = FaceCropper(args.faces_path)
    
    # Verarbeitung starten
    processor = FileProcessor(
        args.base_path, 
        DB_CONFIG, 
        ai_analyzer, 
        annotator,
        face_cropper,
        reanalyze=args.reanalyze  # NEU
    )
    processor.process_all_files(
        limit=args.limit,
        analyze=not args.no_analysis
    )


if __name__ == "__main__":
    main()
